Actions:
--------
count:
------
scala> val rdd = sc.parallelize((1 to 10000).toList)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[172] at parallelize at <console>:24

scala> val totalCount = rdd.count
totalCount: Long = 10000

scala> val filteredRdd = rdd.filter { x => (x % 1000) == 0 }
filteredRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[173] at filter at <console>:26

scala> filteredRdd.count
res109: Long = 10

scala> filteredRdd.collect
res108: Array[Int] = Array(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000)

Ex2:
----
scala> val data = sc.parallelize(List ("AAA","BBB","CCCC","DD","EEEEE","FFF"))
data: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[174] at parallelize at <console>:24

scala> println(data.count)
6

Ex3:
----
scala> val file = sc.textFile("spark/inputs/input2.log")
file: org.apache.spark.rdd.RDD[String] = spark/inputs/input2.log MapPartitionsRDD[176] at textFile at <console>:24

scala> file.collect
res112: Array[String] = Array("spark is in-memory cluster computing system ", "spark is ment for spark processing ", "spark is having diff modules ", "spark is based on rdd processing only ", "thanks saprk ", hadoop and spark are going to lead bigdata market, "spark is in-memory cluster computing system ", "spark is ment for spark processing ", "spark is having diff modules ", "spark is based on rdd processing only ", "thanks saprk ", hadoop and spark are going to lead bigdata market)

scala> val filData = file.filter(x => x.contains("spark"))
filData: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[177] at filter at <console>:26

scala> println(filData.count)
10

countByValue():
---------------
--> The countByValue method returns a count of each unique element in the source RDD. 
--> It returns an instance of the Map class containing each unique element and its count as a key-value pair.

scala> val data = sc.parallelize(List ("AAA","BBB","CCCC","DD","AAA","BBB","AAA","BBB","AAA"))
data: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[178] at parallelize at <console>:24

scala> val countByValueResult = data.countByValue()
countByValueResult: scala.collection.Map[String,Long] = Map(DD -> 1, CCCC -> 1, BBB -> 3, AAA -> 4)


countByKey():
-------------
--> The countByKey method counts the occurrences of each unique key in the source RDD. 
    It returns a Map of key-count pairs.

scala> val iniData = sc.parallelize(List( ("Spark",1),("Scala",2),("Spark",3),("Spark",4),("Spark",8),("Java",9),("Scala",5),("Spark",6) ))
iniData: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[192] at parallelize at <console>:24

scala> val countOfKeys = iniData.countByKey()
countOfKeys: scala.collection.Map[String,Long] = Map(Java -> 1, Scala -> 2, Spark -> 5)

scala> val countOfKeys = iniData.countByValue
countOfKeys: scala.collection.Map[(String, Int),Long] = Map((Spark,1) -> 1, (Spark,4) -> 1, (Scala,5) -> 1, (Java,9) -> 1, 
(Scala,2) -> 1, (Spark,3) -> 1, (Spark,6) -> 1, (Spark,8) -> 1)

scala> val countBykeyResult = data.countByKey
<console>:26: error: value countByKey is not a member of org.apache.spark.rdd.RDD[String]
       val countBykeyResult = data.countByKey
                                   ^

NOTE: countByValue() is suitable for on List(RDDs)/Array(RDDs).
      countByKey() is suitable for on tuple dataset.

first:
------
scala> val data = sc.parallelize(List ("AAA","BBB","CCCC","DD","AAA","BBB","AAA","BBB","AAA"))
data: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[182] at parallelize at <console>:24

scala> val firstElement = data.first
firstElement: String = AAA

max:
----
The max method returns the largest element in an RDD.

scala> val rdd = sc.parallelize(List(45,98,73,61,55,85,91,10))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[183] at parallelize at <console>:24

scala> rdd.max
res113: Int = 98

Ex2:
----
scala> val rdd = sc.parallelize(List("Spark","Spark&Scala","A","Java"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[184] at parallelize at <console>:24

scala> rdd.max
res114: String = Spark&Scala

min:
----
The min method returns the smallest element in an RDD

scala> val rdd = sc.parallelize(List(45,98,73,61,55,85,91,10))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[185] at parallelize at <console>:24

scala> rdd.min
res115: Int = 10

Ex2:
----
scala> val rdd = sc.parallelize(List("Spark","Spark&Scala","A","Java"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[186] at parallelize at <console>:24

scala> rdd.min
res116: String = A


Top():
------
--> The top method takes an integer N as input and returns an array containing the N largest elements in the source RDD.

scala> val rdd = sc.parallelize(List(45,98,73,61,55,85,91,10))
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[187] at parallelize at <console>:24

scala> rdd.top(3)
res117: Array[Int] = Array(98, 91, 85)

Ex2:
----
scala> val rdd = sc.parallelize(List("Spark","Spark&Scala","A","Java"))
rdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[189] at parallelize at <console>:24

scala> rdd.top(2)
res118: Array[String] = Array(Spark&Scala, Spark)


reduce():
---------
--> The higher-order reduce method aggregates the elements of the source RDD using an associative and 
    commutative binary operator provided to it.

scala> val data = sc.parallelize(List(10,20,10,30,50,70))
data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[191] at parallelize at <console>:24

scala> val sum = data.reduce( (x,y) => (x + y) )
sum: Int = 190

scala> val multipliedProduct = data.reduce( (x,y) => (x * y) )
multipliedProduct: Int = 210000000


NOTE: reduce() is an action, reduceByKey() is transformation.

lookup:
-------
scala> val iniData = sc.parallelize(List( ("Spark",1),("Scala",2),("Spark",3),("Spark",4),("Spark",8),("Java",9),("Scala",5),("Spark",6) ))
iniData: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[204] at parallelize at <console>:24

scala> val lookupres = iniData.lookup("spark")
lookupres: Seq[Int] = WrappedArray()

scala> val lookupres = iniData.lookup("Spark")
lookupres: Seq[Int] = WrappedArray(1, 3, 4, 8, 6)



mapPartitionsWithIndex():
-------------------------
scala> val pairRDD = sc.parallelize(List( ("hadoop",2), ("CLOUD", 3),("BI",3), ("SAS", 4),("SALEFORCE", 12), 
("SAP", 12), ("HADOOP", 2)), 2)
pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[209] at parallelize at <console>:24

scala> def myfunc(index:Int,iter:Iterator[(String,Int)]):Iterator[String] = {
     | iter.toList.map(x => "[partID:" + index + ",val:" + x + "]").iterator
     | }
myfunc: (index: Int, iter: Iterator[(String, Int)])Iterator[String]


scala> pairRDD.mapPartitionsWithIndex(myfunc).collect
res119: Array[String] = Array([partID:0,val:(hadoop,2)], [partID:0,val:(CLOUD,3)], [partID:0,val:(BI,3)], [partID:1,val:(SAS,4)], [partID:1,val:(SALEFORCE,12)], [partID:1,val:(SAP,12)], [partID:1,val:(HADOOP,2)])


NOTE: check about this method.


filter:
-------
startsWith():
-------------
scala> val lines = sc.textFile("spark/inputs/input.log")
lines: org.apache.spark.rdd.RDD[String] = spark/inputs/input.log MapPartitionsRDD[214] at textFile at <console>:24

scala> lines.collect
res123: Array[String] = Array(hadoop is one of the bigdata tool, bigdata is not only for hadoop, hadoop is ment for bigdata storage and processing, hadoop is good for analytics also, thanks hadoop)

scala> val res = lines.filter(x => x.startsWith("hadoop"))
res: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[215] at filter at <console>:26

scala> res.collect
res124: Array[String] = Array(hadoop is one of the bigdata tool, hadoop is ment for bigdata storage and processing, hadoop is good for analytics also)

contains():
-----------
scala> res.filter(x => x.contains("bigdata")).collect
res128: Array[String] = Array(hadoop is one of the bigdata tool, hadoop is ment for bigdata storage and processing)

scala> res.map(x => x.contains("bigdata")).collect
res129: Array[Boolean] = Array(true, true, false)

scala> res.filter(x => x.contains("bigdata")).count
res130: Long = 2



