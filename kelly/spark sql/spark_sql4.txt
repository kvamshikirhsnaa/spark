Datasets:
---------
Datasets are similar to RDDs, however, instead of using Java Serialization they use a specialized 
"Encoder to serialize the objects" for processing or transmitting over the network. While both encoders and 
standard serialization are responsible for turning an object into bytes, encoders are code generated dynamically 
and use a format that allows Spark to perform many operations like filtering, sorting and hashing without deserializing 
the bytes back into an object.

scala> val ds = Seq(1, 2, 3).toDS()
ds: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> ds.map(x => x + 1).collect() 
res8: Array[Int] = Array(2, 3, 4)

scala> ds.map(_ + 1).collect() 
res9: Array[Int] = Array(2, 3, 4)


Encoders are also created for case classes:
-------------------------------------------
scala> val jsonDF = spark.read.json("myjson1.json")
jsonDF: org.apache.spark.sql.DataFrame = [age: bigint, city: string ... 1 more field]

scala> jsonDF.show()
+---+---------+-------------+
|age|     city|         name|
+---+---------+-------------+
| 22|     null|   saikrishna|
| 23|     null|      aravind|
| 27| Banglore|vamshikrishna|
| 28|Hydarabad|     narahari|
| 26|     null|      prakash|
+---+---------+-------------+

scala> case class Json1(name:String,age:BigInt,city:String)
defined class Json1

scala> val jsonDS = spark.read.json("myjson1.json").as[Json1]
jsonDS: org.apache.spark.sql.Dataset[Json1] = [age: bigint, city: string ... 1 more field]

scala> jsonDS.show()
+---+---------+-------------+
|age|     city|         name|
+---+---------+-------------+
| 22|     null|   saikrishna|
| 23|     null|      aravind|
| 27| Banglore|vamshikrishna|
| 28|Hydarabad|     narahari|
| 26|     null|      prakash|
+---+---------+-------------+


NOTE: in above we read a json file directly the result will be in DataFrame.
      using case class shema directly from file we created Dataset.


creating DF with out using case classes:
----------------------------------------
Specifying the Schema Programatically

When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, 
or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be 
created programmatically with three steps.
1.	Create an RDD of Rows from the original RDD;
2.	Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.
3.	Apply the schema to the RDD of Rows via createDataFrame method provided by SQLContext

scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
warning: there was one deprecation warning; re-run with -deprecation for details
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@724d4857

scala> val people = sc.textFile("names2")
people: org.apache.spark.rdd.RDD[String] = names2 MapPartitionsRDD[61] at textFile at <console>:24

scala> people.collect
res13: Array[String] = Array(saikrishna,23, aravind,24, prakash,26, vamshikrishna,27, narahari,28)

scala> val schemaString = "name age"
schemaString: String = name age

scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala> import org.apache.spark.sql.types.{StructType,StructField,StringType}
import org.apache.spark.sql.types.{StructType, StructField, StringType}

scala> val schema = StructType(schemaString.split(" ").map(fieldName => StructField(fieldName,StringType,true)))
schmea: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(age,StringType,true))

scala> val rowRDD = people.map(_.split(",")).map(p => Row(p(0), p(1).trim))
rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[63] at map at <console>:28

scala> val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)
peopleDataFrame: org.apache.spark.sql.DataFrame = [name: string, age: string]

scala> peopleDataFrame.registerTempTable("people")
warning: there was one deprecation warning; re-run with -deprecation for details

scala> val results = sqlContext.sql("SELECT name FROM people")
results: org.apache.spark.sql.DataFrame = [name: string]

scala> results.map(t => "Name: " + t(0)).collect().foreach(println)
Name: saikrishna                                                                
Name: aravind
Name: prakash
Name: vamshikrishna
Name: narahari


csv files:
----------
scala> val books = spark.read.csv("book/posts.csv")
books: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 7 more fields]

scala> books.printSchema
root
 |-- _c0: string (nullable = true)
 |-- _c1: string (nullable = true)
 |-- _c2: string (nullable = true)
 |-- _c3: string (nullable = true)
 |-- _c4: string (nullable = true)
 |-- _c5: string (nullable = true)
 |-- _c6: string (nullable = true)
 |-- _c7: string (nullable = true)
 |-- _c8: string (nullable = true)

 scala> books.show(10)
+---+---+-------------------+---+------+---+--------------------+----+---+
|_c0|_c1|                _c2|_c3|   _c4|_c5|                 _c6| _c7|_c8|
+---+---+-------------------+---+------+---+--------------------+----+---+
|  4|  1|2008-07-31 21:42:52|222| 13150|  8|When setting a fo...|  13| 34|
|  6|  1|2008-07-31 22:08:08| 98|  6691|  9|Why doesn't the p...|   5| 12|
|  7|  2|2008-07-31 22:17:57|176|  null|  9|                null|null|  0|
|  8|  1|2008-07-31 23:33:19| 30|  2432|  9|Tool for Converti...|   3|  0|
|  9|  1|2008-07-31 23:40:59|582|133946|  1|How do I calculat...|  37|  4|
| 11|  1|2008-07-31 23:55:37|639| 59517|  1|How do I calculat...|  28|  9|
| 12|  2|2008-07-31 23:56:41|186|  null|  1|                null|null| 11|
| 13|  1|2008-08-01 00:42:38|236| 58340|  9|Determining a web...|  21|  7|
| 14|  1|2008-08-01 00:59:11|149| 37887| 11|Difference betwee...|   5|  1|
| 16|  1|2008-08-01 04:59:33| 29| 41218|  2|Filling a DataSet...|   5|  0|
+---+---+-------------------+---+------+---+--------------------+----+---+

scala> books.orderBy("_c8").show(10)
+---+---+-------------------+---+----+---+--------------------+----+----+
|_c0|_c1|                _c2|_c3| _c4|_c5|                 _c6| _c7| _c8|
+---+---+-------------------+---+----+---+--------------------+----+----+
| 65|  2|2008-08-01 13:20:44|  4|null| 35|                null|null|null|
| 87|  2|2008-08-01 14:26:22|  3|null| 59|                null|null|null|
| 66|  1|2008-08-01 13:20:46| 28|2028| 17|Paging a collecti...|   4|null|
| 39|  1|2008-08-01 12:43:11| 24|2702| 33|Reliable timer in...|   2|null|
| 72|  1|2008-08-01 13:38:27| 12| 765| 25|How do I add exis...|   2|null|
| 52|  2|2008-08-01 13:08:59|  7|null| 23|                null|null|null|
| 73|  2|2008-08-01 13:40:16|  4|null| 13|                null|null|null|
| 62|  2|2008-08-01 13:18:37| 25|null| 45|                null|null|null|
| 78|  2|2008-08-01 13:53:06| 15|null| 55|                null|null|null|
| 81|  2|2008-08-01 14:03:22|  6|null| 41|                null|null|null|
+---+---+-------------------+---+----+---+--------------------+----+----+
only showing top 10 rows


scala> books.orderBy("_c0").show(10)
+-----+---+-------------------+---+------+----+--------------------+----+----+
|  _c0|_c1|                _c2|_c3|   _c4| _c5|                 _c6| _c7| _c8|
+-----+---+-------------------+---+------+----+--------------------+----+----+
|  100|  2|2008-08-01 14:58:15|  8|  null|  59|                null|null|   2|
|10001|  2|2008-08-13 16:05:59| 84|  null|1063|                null|null|   4|
|10006|  1|2008-08-13 16:08:43| 21| 17336| 313|What is the easie...|   6|   1|
|10008|  2|2008-08-13 16:09:09|  4|  null|1109|                null|null|null|
| 1001|  2|2008-08-04 03:17:40|  5|  null|  50|                null|null|   1|
|10020|  2|2008-08-13 16:16:12|  5|  null|1109|                null|null|null|
|10028|  2|2008-08-13 16:19:24|  1|  null| 740|                null|null|null|
|10030|  2|2008-08-13 16:21:37| 12|  null|1109|                null|null|   1|
|10040|  2|2008-08-13 16:28:06| 11|  null|  77|                null|null|   2|
|10042|  1|2008-08-13 16:29:22| 52|188843| 571|How do I implemen...|   6|   1|
+-----+---+-------------------+---+------+----+--------------------+----+----+
only showing top 10 rows

scala> books.groupBy("_c2").count.show
18/09/29 01:17:31 WARN Executor: Managed memory leak detected; size = 8650752 bytes, TID = 80
+-------------------+-----+
|                _c2|count|
+-------------------+-----+
|2008-08-03 22:03:37|    1|
|2008-08-04 03:17:40|    1|
|2008-08-05 06:09:59|    1|
|2008-08-05 18:10:47|    1|
|2008-08-06 01:07:52|    1|
|2008-08-06 18:43:21|    1|
|2008-08-07 05:40:18|    1|
|2008-08-09 02:49:25|    1|
|2008-08-10 09:08:27|    1|
|2008-08-11 20:06:53|    1|
|2008-08-12 09:30:51|    1|
|2008-08-12 11:31:13|    1|
|2008-08-12 16:12:30|    1|
|2008-08-13 01:05:47|    1|
|2008-08-13 05:15:22|    1|
|2008-08-14 00:49:17|    1|
|2008-08-14 17:08:44|    1|
|2008-08-14 22:30:37|    1|
|2008-08-15 03:24:09|    1|
|2008-08-15 14:04:01|    1|
+-------------------+-----+
only showing top 20 rows



creating Dataset from text files:
---------------------------------
scala>  val textFile1 = spark.read.textFile("emp.txt")
textFile1: org.apache.spark.sql.Dataset[String] = [value: string]

scala> textFile1.collect
res43: Array[String] = Array(1000 Gopal 12000, 1001 Krishna 14000, 1002 Ravi 16000, 1002 Ramya 18000, 1003 Rakesh 20000, 1004 Rajesh 22000, 1005 Mounika 24000, 1006 Sravya 26000, 1007 Siya 28000, 1008 Dixitha 30000, 1009 Trinath 34000)

scala> textFile1.printSchema
root
 |-- value: string (nullable = true)


scala> textFile1.show()
+------------------+
|             value|
+------------------+
|  1000 Gopal 12000|
|1001 Krishna 14000|
|   1002 Ravi 16000|
|  1002 Ramya 18000|
| 1003 Rakesh 20000|
| 1004 Rajesh 22000|
|1005 Mounika 24000|
| 1006 Sravya 26000|
|   1007 Siya 28000|
|1008 Dixitha 30000|
|1009 Trinath 34000|
+------------------+

scala> val data = textFile1.map( x => x.split(" "))
data: org.apache.spark.sql.Dataset[Array[String]] = [value: array<string>]

scala> val arr = textFile1.map( x => x.split(" "))
arr: org.apache.spark.sql.Dataset[Array[String]] = [value: array<string>]

scala> val pair = arr.map(x => (x(1).trim,x(2).trim.toInt) )
pair: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]

scala> pair.collect
res1: Array[(String, Int)] = Array((Gopal,12000), (Krishna,14000), (Ravi,16000), (Ramya,18000), (Rakesh,20000), 
(Rajesh,22000), (Mounika,24000), (Sravya,26000), (Siya,28000), (Dixitha,30000), (Trinath,34000))

scala> pair.collect.foreach(println)
(Gopal,12000)
(Krishna,14000)
(Ravi,16000)
(Ramya,18000)
(Rakesh,20000)
(Rajesh,22000)
(Mounika,24000)
(Sravya,26000)
(Siya,28000)
(Dixitha,30000)
(Trinath,34000)

NOTE: we can use RDD apis and Dataset apis on Datasets.




