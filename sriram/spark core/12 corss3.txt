cross:
------
monthly sales report:
---------------------
[edureka_334602@ip-20-0-41-190 ~]$ hadoop fs -cat sparklab/sales.txt
01/01/2015,10000
01/15/2015,20000
02/24/2015,30000
02/07/2015,50000
03/14/2015,60000
03/21/2015,70000
04/03/2015,90000
04/16/2015,80000
05/29/2015,40000
05/30/2015,50000
06/17/2015,70000
06/22/2015,60000
07/07/2015,40000
07/09/2015,50000
08/31/2015,30000
08/15/2015,10000
09/13/2015,5000
09/28/2015,15000
10/11/2015,25000
10/18/2015,30000
11/04/2015,80000
11/23/2015,90000
12/08/2015,100000

scala> val data = sc.textFile("sparklab/sales.txt")
data: org.apache.spark.rdd.RDD[String] = sparklab/sales.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> data.collect
res0: Array[String] = Array(01/01/2015,10000, 01/15/2015,20000, 02/24/2015,30000, 02/07/2015,50000, 
03/14/2015,60000, 03/21/2015,70000, 04/03/2015,90000, 04/16/2015,80000, 05/29/2015,40000, 05/30/2015,50000, 
06/17/2015,70000, 06/22/2015,60000, 07/07/2015,40000, 07/09/2015,50000, 08/31/2015,30000, 08/15/2015,10000, 
09/13/2015,5000, 09/28/2015,15000, 10/11/2015,25000, 10/18/2015,30000, 11/04/2015,80000, 11/23/2015,90000, 
12/08/2015,100000, 12/01/2015,120000)

Exracting month as key and amt as value:
----------------------------------------
scala> val pair = data.map{x =>
     |    val w = x.split(",")
     |    val dt = w(0)
     |    val amt =w(1).toInt
     |    val month = dt.slice(0,2).toInt
     |    (month,amt)
     | }
pair: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[3] at map at <console>:26

scala> pair.collect.foreach(println)
(1,10000)
(1,20000)
(2,30000)
(2,50000)
(3,60000)
(3,70000)
(4,90000)
(4,80000)
(5,40000)
(5,50000)
(6,70000)
(6,60000)
(7,40000)
(7,50000)
(8,30000)
(8,10000)
(9,5000)
(9,15000)
(10,25000)
(10,30000)
(11,80000)
(11,90000)
(12,100000)
(12,120000)

scala> val rep1 = pair.map{ x =>
     |    val month = if(x._1<4) 1 else if(x._1<7) 2 else if(x._1<10) 3 else 4 
     |    val year = x._2
     |    (month,year)
     | }
rep1: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[3] at map at <console>:28

scala> rep1.collect.foreach(println)
(1,10000)
(1,20000)
(1,30000)
(1,50000)
(1,60000)
(1,70000)
(2,90000)
(2,80000)
(2,40000)
(2,50000)
(2,70000)
(2,60000)
(3,40000)
(3,50000)
(3,30000)
(3,10000)
(3,5000)
(3,15000)
(4,25000)
(4,30000)
(4,80000)
(4,90000)
(4,100000)
(4,120000)

scala> val tot1 = rep1.sortByKey().reduceByKey(_+_)
tot1: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[8] at reduceByKey at <console>:30

scala> tot1.collect.foreach(println)
(1,240000)
(2,390000)
(4,445000)
(3,150000)


make copy of tot1:
------------------
scala> val tot2 = tot1
tot2: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[12] at reduceByKey at <console>:30

scala> tot2.collect.foreach(println)
(1,240000)
(2,390000)
(4,445000)
(3,150000)


cartesian tot1, tot2:
---------------------
scala> val cr = tot1.cartesian(tot2)
cr: org.apache.spark.rdd.RDD[((Int, Int), (Int, Int))] = CartesianRDD[13] at cartesian at <console>:34

scala> cr.collect.foreach(println)
((1,240000),(1,240000))
((1,240000),(2,390000))
((2,390000),(1,240000))
((2,390000),(2,390000))
((1,240000),(4,445000))
((1,240000),(3,150000))
((2,390000),(4,445000))
((2,390000),(3,150000))
((4,445000),(1,240000))
((4,445000),(2,390000))
((3,150000),(1,240000))
((3,150000),(2,390000))
((4,445000),(4,445000))
((4,445000),(3,150000))
((3,150000),(4,445000))
((3,150000),(3,150000))


make into single tuple:
-----------------------
scala> val tup = cr.map{x =>
     |    val t1 = x._1
     |    val t2 = x._2
     |    val m1 = t1._1
     |    val yr1 = t1._2
     |    val m2 = t2._1
     |    val yr2 = t2._2
     |    (m1,m2,yr1,yr2)
     | }
tup: org.apache.spark.rdd.RDD[(Int, Int, Int, Int)] = MapPartitionsRDD[14] at map at <console>:36

scala> tup.collect.foreach(println)
(1,1,240000,240000)
(1,2,240000,390000)
(2,1,390000,240000)
(2,2,390000,390000)
(1,4,240000,445000)
(1,3,240000,150000)
(2,4,390000,445000)
(2,3,390000,150000)
(4,1,445000,240000)
(4,2,445000,390000)
(3,1,150000,240000)
(3,2,150000,390000)
(4,4,445000,445000)
(4,3,445000,150000)
(3,4,150000,445000)
(3,3,150000,150000)


scala> val res = tup.filter(x => x._1-x._2 == 1)
res: org.apache.spark.rdd.RDD[(Int, Int, Int, Int)] = MapPartitionsRDD[15] at filter at <console>:38

scala> res.collect.foreach(println)
(2,1,390000,240000)
(3,2,150000,390000)
(4,3,445000,150000)


scala> val salesrep = res.map{ x =>
     |     val m1 = x._1
     |     val m2 = x._2
     |     val yr1 = x._3
     |     val yr2 = x._4
     |     val pgrowth = ((yr1-yr2)*100)/yr2             // ((current yr sales-prev yr sales)*100)/prev yr sales
     |     (m1,m2,yr1,yr2,pgrowth)
     | }
salesrep: org.apache.spark.rdd.RDD[(Int, Int, Int, Int, Int)] = MapPartitionsRDD[16] at map at <console>:40

scala> salesrep.collect.foreach(println)
(2,1,390000,240000,62)
(3,2,150000,390000,-61)
(4,3,445000,150000,196)

NOTE: from above results
      In 2nd querter monthly sales are 62% increased compare to 1st querter monthly sales.
      In 3rd querter monthly sales are 61% decreased compare to 2nd querter monthly sales.
      In 4th querter monthly sales are 196% increased compare to 3nd querter monthly sales.


convert salesrep from tuple shape to String and save in Hdfs:
-------------------------------------------------------------
scala> val finalrep = salesrep.map(x => x._1+" "+x._2+" "+x._3+" "+x._4+" "+x._5)
finalrep: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[18] at map at <console>:42

scala> finalrep.collect.foreach(println)
2 1 390000 240000 62
3 2 150000 390000 -61
4 3 445000 150000 196

scala> finalrep.saveAsTextFile("sparklab/salereport")


























