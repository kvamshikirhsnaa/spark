exam:
-----
scala> val tokenized = sc.textFile("sparklab/input.txt").flatMap(x => x.split(" "))
tokenized: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:24

scala> tokenized.collect.foreach(println)
hadoop
is
having
good
market
good
people
learn
hadoop
having
hadoop
knowledge
is
good
market
is
good
now
is
hadoop
the
market
leader
now
hadoop
is
---
---
---

scala> val wordCount = tokenized.map(x => (x,1)).reduceByKey(_+_)
wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:26

scala> val filtered = wordCount.filter(x => x._2 >= 3)
filtered: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[5] at filter at <console>:28

scala> val charCounts = filtered.flatMap(x => x._1.toCharArray).map(x => (x,1)).reduceByKey(_+_)
charCounts: org.apache.spark.rdd.RDD[(Char, Int)] = ShuffledRDD[8] at reduceByKey at <console>:30

scala> charCounts.toDebugString
res0: String =
(2) ShuffledRDD[8] at reduceByKey at <console>:30 []
 +-(2) MapPartitionsRDD[7] at map at <console>:30 []
    |  MapPartitionsRDD[6] at flatMap at <console>:30 []
    |  MapPartitionsRDD[5] at filter at <console>:28 []
    |  ShuffledRDD[4] at reduceByKey at <console>:26 []
    +-(2) MapPartitionsRDD[3] at map at <console>:26 []
       |  MapPartitionsRDD[2] at flatMap at <console>:24 []
       |  sparklab/input.txt MapPartitionsRDD[1] at textFile at <console>:24 []
       |  sparklab/input.txt HadoopRDD[0] at textFile at <console>:24 []


NOTE: 2 stages for 2 reduceByKey operations.

scala> charCounts.collect.foreach(println)
(d,4)                                                                           
(p,3)
(t,2)
(h,3)
(n,4)
(v,1)
(r,3)
(l,4)
(w,2)
(s,1)
(e,9)
(a,5)
(i,2)
(k,2)
(o,7)
(g,3)
(m,1)
